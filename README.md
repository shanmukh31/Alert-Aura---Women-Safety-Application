
# ğŸš¨ Alert Aura â€“ AI-Powered Women Safety Application

> An autonomous, real-time threat detection system integrating gesture recognition, speech emotion detection, crime zone awareness, and sensor-based anomaly detection.

---

## ğŸ“Œ Overview

**Alert Aura** is an AI-driven multimodal safety system developed to **autonomously detect and respond to emergencies**, particularly focused on enhancing women's safety. Unlike traditional panic-button systems, it **requires no user intervention**, functioning in real-time through camera input, voice emotion, sensor motion, and geolocation-based crime risk.

Developed using **Python, OpenCV, MediaPipe, PyTorch, Librosa**, and other tools, it fuses models for:
- Hand gesture recognition (fist = SOS)
- Emotion detection from speech (e.g., fear, panic)
- Panic keyword detection via speech-to-text
- Decibel-based threat alerts
- Sensor-based motion analysis
- Crime dataâ€“based location risk prediction

---

## ğŸ§  Models & Modules

### 1. ğŸ¤ Speech Emotion Recognition Model (LSTM)
- **Model**: LSTM-based deep learning classifier using the TESS dataset.
- **Input**: Real-time audio from the microphone.
- **Features**: MFCCs extracted using `librosa`.
- **Output**: One of 7 emotions (e.g., Fear, Sad, Happy) + confidence score.
- **Trigger**: Panic-related emotions initiate SOS alert.

> ğŸ§ª Accuracy: **93.21%**

### 2. ğŸ”Š Speech-to-Text & SOS Keyword Detection (Whisper)
- **Model**: OpenAI's `whisper` ASR model.
- **Input**: 5-second audio clips.
- **Detection**: Flags phrases like "help", "save me", or "danger".
- **Output**: If such keywords are found â†’ trigger alert.

> âš ï¸ Fast response to verbal cues even in background noise.

### 3. âœŠ Gesture Recognition Model (MediaPipe)
- **Framework**: Google's MediaPipe (pretrained).
- **Logic**: Rule-based detection of **fist** gesture using landmark analysis.
- **Output**: Activates full SOS pipeline.
- **Customization**: User can specify missing fingers (e.g., thumb amputee).

> ğŸ‘â€ğŸ—¨ Always-on video monitoring with minimal CPU usage.

### 4. ğŸ“ Location Risk Predictor
- **Logic**: Matches userâ€™s GPS coordinates to a dataset of **Telangana crime statistics (2022)**.
- **Zones**:
  - Red â†’ High crime
  - Orange â†’ Medium
  - Green â†’ Low
- **Output**: Displays zone and influences threat level calculation.

> ğŸ—ºï¸ Offline matching for fast and private use.

### 5. ğŸŒ€ Sensor Activity Recognition (Liquid Neural Network)
- **Input**: Accelerometer & gyroscope data.
- **Model**: Liquid Neural Network-based classifier.
- **Classes**: Sitting, walking, running, sleeping, anomaly.
- **Output**: Detects sudden or erratic movement â†’ triggers alert.

> ğŸ” Accuracy: **78.05%**, supports on-device edge computation.

### 6. ğŸ“¢ Decibel Level Meter
- **Function**: Detects unusually loud noise levels (e.g., screaming).
- **Threshold**: >60 dB triggers threat evaluation.
- **Output**: Visual + logical input to SOS handler.

### 7. ğŸ”— Integrated Decision Engine
- **Input Sources**: Audio emotion, keyword, gesture, motion, decibel, location zone.
- **Logic**: Rule-based system (if-else logic).
- **Triggers**:
  - Send Whatsapp Alerts to emergency contact.
  - Start **video + audio recording** for 5 seconds.
  - Flash SOS warning on-screen.

> ğŸ” Modular design â†’ Each model operates independently but reports to a central decision unit.

---

## ğŸ› ï¸ Tech Stack

| Component         | Tool/Library                          |
|------------------|----------------------------------------|
| Language          | Python 3.10+                           |
| UI Framework      | Tkinter                                |
| ML Libraries      | PyTorch, TensorFlow, Keras, Scikit-learn |
| Audio Processing  | Librosa, SoundDevice                   |
| Video Processing  | OpenCV, MediaPipe, FFmpeg              |
| ASR               | Whisper (OpenAI)                       |
| Geolocation       | Geocoder, Google Maps API (optional)   |
| Dataset           | TESS (speech), Telangana Crime Stats   |

---

## ğŸ’» Installation & Setup

1. Clone the repo:
   ```bash
   git clone https://github.com/yourusername/alert-aura.git
   cd alert-aura
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv aura_env
   source aura_env/bin/activate  # On Windows: aura_env\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Download and place the required models:
   - `lstm_model2.pth1` (emotion)
   - `liquid_model` (sensor)
   - Whisper base model will auto-download on first run.

5. Run the integrated application:
   ```bash
   python main_gui.py
   ```

---

## ğŸ¯ Key Features

- Real-time, multi-modal threat detection
- No internet needed for core functions
- Location-based crime zone awareness
- Video/audio evidence auto-capture
- Customization for physical impairments (missing fingers)
- Edge-device optimized & battery-conscious

---

## ğŸ“ˆ Results Snapshot

- Emotion Detection Accuracy: **93.21%**
- Sensor Activity Classification: **78.05%**
- Gesture Detection (Fist): **>95% in bright lighting**
- Keyword Detection (Whisper): **~98% precision**

## ğŸ§‘â€ğŸ’» Authors

- **N. Sushanth** 
- **D. Shanmukhaditya**  
- **R. Venkata Anirudh** 

---

## ğŸ“š References

- [TESS Dataset](https://tspace.library.utoronto.ca/handle/1807/24487)  
- [MediaPipe Hands](https://google.github.io/mediapipe/)  
- [Whisper ASR](https://github.com/openai/whisper)  
- [Crime Data Source â€“ Govt of India Open Data Portal](https://data.gov.in)
